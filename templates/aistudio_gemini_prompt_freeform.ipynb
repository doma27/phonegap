{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doma27/phonegap/blob/master/templates/aistudio_gemini_prompt_freeform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlE8UqxrDIez"
      },
      "source": [
        "### Install & import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kWIuwKG2_oWE"
      },
      "outputs": [],
      "source": [
        "# Install the client library and import necessary modules.\n",
        "import google.generativeai as genai\n",
        "\n",
        "import base64\n",
        "import copy\n",
        "import hashlib\n",
        "import io\n",
        "import json\n",
        "import mimetypes\n",
        "import pathlib\n",
        "import pprint\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fet3lFjdKHEM"
      },
      "source": [
        "## Set the API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoRWILAtCzBE"
      },
      "source": [
        "Add your API_KEY to the secrets manager in the left panel \"üîë\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LaLCwNlkCyQd"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvYoR3WCeKr"
      },
      "outputs": [],
      "source": [
        "# Configure the client library by providing your API key.\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weo-o73WDpdm"
      },
      "source": [
        "## Parse the arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uIog-0SyDuIF"
      },
      "outputs": [],
      "source": [
        "model = 'gemini-1.5-pro-002' # @param {isTemplate: true}\n",
        "contents_b64 = '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' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MS4xNSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RkbnEEIFEHFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba8efca-0355-4aa7-f1f9-d2cb98c9a79e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'parts': [{'text': 'model = genai.GenerativeModel(\\'models/gemini-1.5-pro-002\\')\\\\nresponse = model.generate_content(contents=\\\\\"Who won Wimbledon this year?\\\\\",\\\\n                                  tools=\\'google_search_retrieval\\')\\\\nprint(response)'}]},\n",
              " {'role': 'model',\n",
              "  'parts': [{'text': 'You\\'re trying to use the `genai` library to get the Wimbledon winner using Google Search. However, the code snippet you provided is incomplete and needs some adjustments to function correctly. Here\\'s how you can fix it and achieve the desired result:\\\\n\\\\n```python\\\\nimport genai\\\\n\\\\n# Assuming you have the \\'genai\\' library installed\\\\nmodel = genai.GenerativeModel(\\'models/gemini-1.5-pro-002\\')\\\\n\\\\n# Define the prompt\\\\nprompt = \\\\\"Who won Wimbledon this year?\\\\\"\\\\n\\\\n# Use the \\'google_search_retrieval\\' tool\\\\nresponse = model.generate_content(\\\\n    contents=prompt,\\\\n    tools=\\'google_search_retrieval\\'\\\\n)\\\\n\\\\n# Print the extracted answer\\\\nprint(response)\\\\n```\\\\n\\\\n**Explanation:**\\\\n\\\\n1. **Import the `genai` Library:** Ensure you have the `genai` library installed. If not, use `pip install genai`.\\\\n2. **Create a Generative Model:** You initialize a `GenerativeModel` object with the desired model. \\\\n3. **Define the Prompt:** Clearly state your question in a string variable.\\\\n4. **Use the `google_search_retrieval` Tool:**  The `tools` argument specifies that you want to use Google Search for retrieval.\\\\n5. **Generate Content:**  Call the `generate_content` method to trigger the model and retrieval process.\\\\n6. **Print the Response:** The `response` object will contain the extracted answer from Google Search, which you can then print.\\\\n\\\\n**Important Considerations:**\\\\n\\\\n* **Authentication:** You might need to set up authentication with Google Search API for this to work properly. Refer to the `genai` documentation for guidance.\\\\n* **Model Availability:** Ensure you have access to the `gemini-1.5-pro-002` model or a compatible alternative.\\\\n* **API Keys:**  If you are using the `genai` library, you will need to set up API keys to access the Gemini model.  The \\'genai\\' documentation will provide details on how to obtain and set up the necessary keys.\\\\n\\\\nBy making these adjustments, you should be able to query Google Search and retrieve the Wimbledon winner from the most relevant results.\\\\n'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "gais_contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ca3e641ee9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e98262-883a-441a-ba8d-af4fc48929d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'temperature': 1.15, 'top_p': 0.95, 'top_k': 40, 'max_output_tokens': 8192}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "generation_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "11ce12f5bbac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff4a2ab-86c9-411b-8c49-725b314b800a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "safety_settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoL3p3KPylFW"
      },
      "outputs": [],
      "source": [
        "# @title `show_file` function\n",
        "drive = None\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"üìÅ Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9D5UdhL8MTk"
      },
      "outputs": [],
      "source": [
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F6SKiuo88LQ"
      },
      "source": [
        "## Convert & upload files\n",
        "\n",
        "For each file, Google AI Studio either sent:\n",
        "- a Google Drive IDs\n",
        "- a URL, or\n",
        "- the raw bytes (`inline_data`).\n",
        "\n",
        "The API itself onlty understands two ways of sending files:\n",
        "\n",
        "- `inline_data` - where the bytes are placed inline in the request.\n",
        "- `file_data` - where the file is uploaded to the Files API, and you pass a reference to that file.\n",
        "\n",
        "This section goes through the `contents` from Google AI Studio, and uploads the file data, to the Files API, so Gemini can access it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wJAs_ZfuwCq"
      },
      "outputs": [],
      "source": [
        "# @title `upload_file` function\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        hash = hashlib.sha256(data).hexdigest()\n",
        "        path = tempfiles / hash\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"üìÅ Files\"in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TehY-utE3OR"
      },
      "outputs": [],
      "source": [
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUFIM-r39cuc"
      },
      "source": [
        "Here is the coverted `Content`s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ-sPFRSxdQg"
      },
      "outputs": [],
      "source": [
        "contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB2LxPmAB95V"
      },
      "outputs": [],
      "source": [
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm3RXwYuGtZK"
      },
      "outputs": [],
      "source": [
        "if generation_config.get(\"candidate_count\", 1) == 1:\n",
        "    display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjT4jtJc2aAk"
      },
      "outputs": [],
      "source": [
        "response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1YBkS4MV8L"
      },
      "source": [
        "## Or Create a chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOd-4IHUqx4R"
      },
      "outputs": [],
      "source": [
        "gemini = genai.GenerativeModel(\n",
        "    model_name=model,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOPCBs2grsIt"
      },
      "source": [
        "A `ChatSession`, should always end with the model's turn.\n",
        "\n",
        "So before creating the `chat` check whos turn was last.\n",
        "\n",
        "If the user was last, attach all but the last content as the `history` and send the last content with `send_message` to get the model's response.\n",
        "\n",
        "If the model was last, put the whole contents list in the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIbdDOeFqyiE"
      },
      "outputs": [],
      "source": [
        "model_turn = contents[-1].get(\"role\", None) == \"user\"\n",
        "\n",
        "if model_turn:\n",
        "    chat = gemini.start_chat(history=contents[:-1])\n",
        "\n",
        "    response = chat.send_message(contents[-1])\n",
        "\n",
        "    if generation_config.get(\"candidate_count\", 1) == 1:\n",
        "        display(Markdown(response.text))\n",
        "else:\n",
        "    chat = gemini.start_chat(history=contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gWBb5UtOL5M"
      },
      "outputs": [],
      "source": [
        "if model_turn:\n",
        "    response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdz66HVvsnRc"
      },
      "source": [
        "After that use `send_message` to continue the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb87HPvBrx29"
      },
      "outputs": [],
      "source": [
        "# response = chat.send_message(\"Interesting, tell me more.\")\n",
        "# display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}